{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necssary Libraries\n",
    "import numpy as np\n",
    "import librosa\n",
    "import sounddevice as sd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class handles audio feature extraction and real-time audio recording.\n",
    "class AudioProcessor:\n",
    "    def __init__(self, sample_rate=16000):\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "    def extract_features(self, audio):\n",
    "        # Extract MFCC features and their delta (change over time) and delta-delta (second-order change)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=self.sample_rate, n_mfcc=20)\n",
    "        delta = librosa.feature.delta(mfccs)\n",
    "        delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "        features = np.concatenate([mfccs, delta, delta2])  # Combine all features\n",
    "        return features\n",
    "\n",
    "    def record_audio(self, duration=5):\n",
    "        # Record audio for the specified duration\n",
    "        recording = sd.rec(int(duration * self.sample_rate), \n",
    "                         samplerate=self.sample_rate, channels=1)\n",
    "        sd.wait()\n",
    "        return recording.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerVerificationModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 2)  # Output: Binary classification (target or non-target)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # Process input through LSTM\n",
    "        output = self.fc(lstm_out[:, -1, :])  # Use the last timestep for prediction\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerVerifier:\n",
    "    def __init__(self, sample_rate=16000):\n",
    "        self.processor = AudioProcessor(sample_rate)\n",
    "        self.model = None\n",
    "        \n",
    "    def train(self, target_recordings, non_target_recordings, epochs=50):\n",
    "        # Extract features and labels for target and non-target recordings\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        for audio in target_recordings:\n",
    "            feat = self.processor.extract_features(audio)\n",
    "            features.append(feat.T)\n",
    "            labels.append(1)  # Label for target speakers\n",
    "            \n",
    "        for audio in non_target_recordings:\n",
    "            feat = self.processor.extract_features(audio)\n",
    "            features.append(feat.T)\n",
    "            labels.append(0)  # Label for non-target speakers\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = SpeakerDataset(features, labels)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        # Initialize model\n",
    "        input_size = features[0].shape[1]  # Feature dimension\n",
    "        self.model = SpeakerVerificationModel(input_size)\n",
    "        \n",
    "        # Training setup\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        \n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch_features, batch_labels in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_features)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "            if (epoch + 1) % 10 == 0:  # Print loss every 10 epochs\n",
    "                print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}')\n",
    "    \n",
    "    def verify_speaker(self, audio, threshold=0.5):\n",
    "        # Verify if the given audio is the target speaker\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            features = self.processor.extract_features(audio)\n",
    "            features = torch.FloatTensor(features.T).unsqueeze(0)\n",
    "            output = torch.softmax(self.model(features), dim=1)\n",
    "            probability = output[0][1].item()  # Probability of being target speaker\n",
    "            return probability > threshold, probability\n",
    "    \n",
    "    def real_time_verification(self, duration=5):\n",
    "        print(\"Press 'q' and hit Enter to quit real-time verification.\")\n",
    "        while True:\n",
    "            # Prompt the user to continue or exit\n",
    "            user_input = input(\"Press Enter to record, or 'q' to quit: \").strip().lower()\n",
    "            if user_input == 'q':\n",
    "                print(\"Exiting real-time verification.\")\n",
    "                break\n",
    "            \n",
    "            print(\"Recording...\")\n",
    "            audio = self.processor.record_audio(duration)\n",
    "            is_target, confidence = self.verify_speaker(audio)\n",
    "            result = \"Target\" if is_target else \"Non-target\"\n",
    "            print(f\"Speaker: {result} (confidence: {confidence:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_system(verifier, test_target, test_non_target):\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    for audio in test_target:\n",
    "        is_target, _ = verifier.verify_speaker(audio)\n",
    "        true_labels.append(1)\n",
    "        pred_labels.append(1 if is_target else 0)\n",
    "    \n",
    "    for audio in test_non_target:\n",
    "        is_target, _ = verifier.verify_speaker(audio)\n",
    "        true_labels.append(0)\n",
    "        pred_labels.append(1 if is_target else 0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(true_labels, pred_labels),\n",
    "        'f1': f1_score(true_labels, pred_labels)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.6750\n",
      "Epoch 20, Loss: 0.6198\n",
      "Epoch 30, Loss: 0.5007\n",
      "Epoch 40, Loss: 0.3295\n",
      "Epoch 50, Loss: 0.1962\n",
      "Press 'q' and hit Enter to quit real-time verification.\n",
      "Recording...\n",
      "Speaker: Non-target (confidence: 0.46)\n",
      "Exiting real-time verification.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    verifier = SpeakerVerifier()\n",
    "    \n",
    "    # Simulated training data (replace with real data for actual use)\n",
    "    target_recordings = [np.random.randn(16000*5) for _ in range(10)]\n",
    "    non_target_recordings = [np.random.randn(16000*5) for _ in range(10)]\n",
    "    \n",
    "    # Train the model\n",
    "    verifier.train(target_recordings, non_target_recordings)\n",
    "    \n",
    "    # Start real-time verification (comment this out if not testing real-time)\n",
    "    verifier.real_time_verification()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
